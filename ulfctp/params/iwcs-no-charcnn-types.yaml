# File for development of the arcmap data.

# Device-sensitive parameters
ulf_data_dir: &ulf_data_dir ulfdata/5-27-2020-arcmap
oracle_dir: &ulf_oracle_dir ulfdata/oracle/5-27-2020-arcmap-train_cache2
serialization_dir: &serialization_dir ckpt-iwcs-no-charcnn-types
glove: &glove data/glove/glove.840B.300d.zip

# Model parameters
model:
  model_type: ULFCTP
  use_char_cnn: False
  use_pos: True
  use_ts_feats: True
  # Coverage loss (reduce repetition in nodes)
  use_coverage: True
  use_bert: False
  use_roberta: &use_roberta True
  # Whether the buffer contains concepts or words (this should correspond with
  # the transition_system.unreserve_gen_method parameter.
  use_concept_inputs: False
  # Maximum number of concepts generated.
  #max_decode_length: 80
  # Maximum number of actions taken by transition system (both training and decoding).
  max_action_length: 800
  # Top-k actions to consider during beam search.
  # NB: Since some actions generated by the model may be invalid for the 
  #     transition system state and therefore be ignored, it's a good idea
  #     to make this larger than the beam size.
  topk_size: 155
  beam_size: 3

  transition_system:
    cache_size: 2
    reserve_gen_method: promote
    unreserve_gen_method: word
    focus_method: cache
    type_checking_method: none
    promote_symbol_file: data_processing/arcmap_promote_symbols
    inseq_symbol_file: data_processing/arcmap_inseq_symbols
    action_counter_dir: *ulf_oracle_dir
    arc_choices_dir: *ulf_oracle_dir
    lexicon_file: lexicon/5-27-2020-full.json
    strict_lexicon: True
    buffer_offset: 1
    action_repeat_limits:
      promote: 3
      seqgen: 3

  bert:
    pretrained_model_dir: data/bert-base-cased
    hidden_size: 768

  roberta:
    pretrained_model_dir: data/roberta-base
    hidden_size: 768

  encoder_token_embedding:
    num_embeddings:
    vocab_namespace: 'encoder_token_ids'
    embedding_dim: 300
    padding_index: 0
    dropout: 0.33
    pretrained_file: *glove

  encoder_pos_embedding:
    num_embeddings:
    vocab_namespace: 'pos_tags'
    embedding_dim: 100
    padding_index: 0
    dropout: 0.33

  encoder_lemma_embedding:
    num_embeddings:
    vocab_namespace: 'src_lemma_tags'
    embedding_dim: 100
    padding_index: 0
    dropout: 0.33

  encoder_char_embedding:
    num_embeddings:
    vocab_namespace: 'encoder_token_characters'
    embedding_dim: &encoder_char_embedding_dim 100
    padding_index: 0
    dropout: 0.33

  encoder_char_cnn:
    embedding_dim: *encoder_char_embedding_dim
    num_filters: 100
    ngram_filter_sizes: [3]

  decoder_token_embedding:
    num_embeddings:
    vocab_namespace: 'decoder_token_ids'
    embedding_dim: 300
    padding_index: 0
    dropout: 0.33
    pretrained_file: *glove
    data_type: ULF

  decoder_pos_embedding:
    num_embeddings:
    vocab_namespace: 'pos_tags'
    embedding_dim: 100
    padding_index: 0
    dropout: 0.33

  decoder_coref_embedding:
    num_embeddings: 500
    embedding_dim: 50
    padding_index: 0
    dropout: 0.33

  decoder_char_embedding:
    num_embeddings:
    vocab_namespace: 'decoder_token_characters'
    embedding_dim: &decoder_char_embedding_dim 100
    padding_index: 0
    dropout: 0.33

  decoder_char_cnn:
    embedding_dim: *decoder_char_embedding_dim
    num_filters: 100
    ngram_filter_sizes: [3]

  action_embedding:
    num_embeddings:
    vocab_namespace: 'action_token_ids'
    embedding_dim: 100
    padding_index: 0
    dropout: 0.33

  feat_embedding:
    num_embeddings:
    vocab_namespace: 'action_feature_token_ids'
    embedding_dim: &feat_embedding_dim 25
    padding_index: 0
    dropout: 0.33

  encoder:
    input_size:
    hidden_size: &encoder_hidden_size 256
    num_layers: 3
    use_highway: False
    dropout: 0.33

  decoder:
    input_size:
    hidden_size: &decoder_hidden_size 128
    num_layers: 2
    use_highway: False
    dropout: 0.33

  source_attention:
    attention_function: mlp
    coverage: True

  coref_attention:
    attention_function: mlp
    share_linear: True

  hard_attn_decoder:
    hard_attn_encoder:
      input_size:
      hidden_size: 256
      num_layers: 2
      use_highway: False
      dropout: 0.33

    mlp_decoder:
      hidden_dim: 256
      n_layers: 1
      dropout: 0.33

    # is this even used?
    decode_algorithm: 'greedy'
    word_input_size: *encoder_hidden_size
    concept_input_size: *decoder_hidden_size
    ts_feature_input_size: *feat_embedding_dim

  mimick_test:
    data: ulfdata/5-27-2020-arcmap/dev/oracle_examples.json
    # ULF_SYMBOL_PRED uses predicted concepts for decoding
    # ULF uses the gold concept sequence for decoding
    data_type: ULF_SYMBOL_PRED
    serialization_dir: *serialization_dir
    prediction_basefile: dev.pred.txt
    batch_size: 2  # Decoding has to be done one at a time anyway.
    el_smatch_eval_script: ulfctp/scripts/el_smatch_eval_test.sh
    smatch_eval_script: ulfctp/scripts/smatch_eval_test.sh
    sembleu_eval_script: ulfctp/scripts/sembleu_eval_test.sh
    smatch_dir: ulfctp/tools/amr-evaluation-tool-enhanced
    word_splitter: &word_splitter data/bert-base-cased/bert-base-cased-vocab.txt
    use_roberta: *use_roberta


# Vocabulary
vocab:
  non_padded_namespaces: [coref_tags]
  min_count:
      encoder_token_ids: 1
      decoder_token_ids: 1
  max_vocab_size:
      encoder_token_ids: 9200
      decoder_token_ids: 7300


# Data parameters
data:
  data_dir: *ulf_data_dir
  train_data: train/oracle_examples.json
  dev_data: dev/oracle_examples.json
  test_data: test/oracle_examples.json
  data_type: ULF
  batch_first: True
  iterator:
    train_batch_size: &train_batch_size 32
    test_batch_size: 32
    iter_type: BucketIterator
    sorting_keys:  # (field, padding type)
      - [tgt_tokens, num_tokens]
  word_splitter: *word_splitter
  use_roberta: *use_roberta


# Training parameters
environment:
  recover: False
  seed: 1
  numpy_seed: 1
  torch_seed: 1
  serialization_dir: *serialization_dir
  file_friendly_logging: False
  gpu: True
  cuda_device: 0
  # If true, runs slower but replicable even on CuDNN backend.
  deterministic: False

trainer:
  device:
  no_grad: []
  optimizer_type: adam
  learning_rate: 0.001
  max_grad_norm: 5.0
  batch_size: *train_batch_size
  shuffle: True
  epochs: 25
  mimick_epoch: 8
  dev_metric: +SEMBLEU_PREC
  serialization_dir: *serialization_dir
  model_save_interval:

test:
  evaluate_on_test: True
  data: ulfdata/5-27-2020-arcmap/test/oracle_examples.json
  prediction_basefile: test.pred.txt

fine-tuning:
  type_checking_method: composition
  beam_size: 10
  best_n: 5
